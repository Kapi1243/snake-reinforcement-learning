{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f1bb877",
   "metadata": {},
   "source": [
    "# ðŸ Snake Q-Learning Tutorial\n",
    "\n",
    "This notebook demonstrates how to train a reinforcement learning agent to play Snake using Q-Learning.\n",
    "\n",
    "## What You'll Learn\n",
    "- How the Snake game environment works\n",
    "- Q-Learning fundamentals\n",
    "- Training and evaluating an RL agent\n",
    "- Visualizing learning progress"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a34870e",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ffd8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "# Add src directory to path\n",
    "sys.path.insert(0, '../src')\n",
    "\n",
    "from Snake import SnakeGame, Snake\n",
    "from QLearningAgent import QLearningAgent\n",
    "from config import PresetConfigs\n",
    "from utils import plot_training_curves, analyze_state_coverage, print_training_summary\n",
    "\n",
    "print(\"âœ… All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745267d1",
   "metadata": {},
   "source": [
    "## 2. Explore the Environment\n",
    "\n",
    "Let's create a game and see how the state representation works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d645540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small game\n",
    "game = SnakeGame(width=8, height=8)\n",
    "\n",
    "print(\"Initial Game State:\")\n",
    "game.display()\n",
    "\n",
    "print(f\"\\nSnake length: {game.length}\")\n",
    "print(f\"State vector: {game.state}\")\n",
    "print(f\"State number: {game._get_state_number()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8df6aa0",
   "metadata": {},
   "source": [
    "### Understanding the State Vector\n",
    "\n",
    "The state is an 8-element binary vector:\n",
    "- **Bits 0-3**: Obstacles in each direction (â†‘â†’â†“â†)\n",
    "- **Bits 4-7**: Food direction (â†‘â†’â†“â†)\n",
    "\n",
    "Let's make a few moves and see how the state changes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba28e445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a random move\n",
    "import random\n",
    "\n",
    "action = random.randint(0, 3)\n",
    "direction_names = [\"UP\", \"RIGHT\", \"DOWN\", \"LEFT\"]\n",
    "\n",
    "print(f\"Taking action: {direction_names[action]}\")\n",
    "state, reward, game_over, score = game.step(action)\n",
    "\n",
    "game.display()\n",
    "print(f\"\\nReward: {reward}\")\n",
    "print(f\"New state: {game.state}\")\n",
    "print(f\"Score: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098808cf",
   "metadata": {},
   "source": [
    "## 3. Create and Configure an Agent\n",
    "\n",
    "Now let's create a Q-Learning agent with custom hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aaae72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create agent with default parameters\n",
    "agent = QLearningAgent(\n",
    "    gamma=0.8,          # Discount factor - how much to value future rewards\n",
    "    epsilon=0.2,        # Exploration rate - 20% random actions during training\n",
    "    learning_rate=0.9   # Learning rate for Q-value updates\n",
    ")\n",
    "\n",
    "print(\"Agent created!\")\n",
    "print(f\"Q-table shape: {agent.q_table.shape}\")\n",
    "print(f\"Number of states: {agent.num_states}\")\n",
    "print(f\"Number of actions: {agent.num_actions}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0e386a",
   "metadata": {},
   "source": [
    "## 4. Train the Agent\n",
    "\n",
    "Let's train the agent for 5000 episodes. This should take a few minutes.\n",
    "\n",
    "**Note**: Set `num_episodes` to 1000 for a quicker demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8252302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "BOARD_SIZE = 16\n",
    "NUM_EPISODES = 5000  # Change to 1000 for quick demo\n",
    "EVAL_FREQUENCY = 100\n",
    "\n",
    "print(f\"Starting training for {NUM_EPISODES} episodes...\\n\")\n",
    "\n",
    "training_results = agent.train(\n",
    "    board_size=BOARD_SIZE,\n",
    "    num_episodes=NUM_EPISODES,\n",
    "    eval_frequency=EVAL_FREQUENCY,\n",
    "    eval_episodes=25,\n",
    "    save_q_history=False,  # Set to True to save convergence data\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723ac9e",
   "metadata": {},
   "source": [
    "## 5. Analyze Training Results\n",
    "\n",
    "Let's visualize the learning progress."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7225c6c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training progress\n",
    "agent.plot_training_progress(save_path='../results/notebook_training_progress.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a164f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze state space coverage\n",
    "coverage = analyze_state_coverage(agent.q_table)\n",
    "\n",
    "print(\"State Space Analysis:\")\n",
    "print(f\"Total possible states: {coverage['total_states']}\")\n",
    "print(f\"States visited: {coverage['visited_states']} ({coverage['visit_percentage']:.1f}%)\")\n",
    "print(f\"Fully explored states: {coverage['fully_explored_states']} ({coverage['full_exploration_percentage']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351b0bb",
   "metadata": {},
   "source": [
    "## 6. Evaluate the Trained Agent\n",
    "\n",
    "Let's test how well our agent performs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7af5212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate over 100 games\n",
    "avg_score, scores = agent.evaluate(board_size=BOARD_SIZE, num_episodes=100)\n",
    "\n",
    "print(\"Evaluation Results (100 games):\")\n",
    "print(f\"Average score: {avg_score:.2f}\")\n",
    "print(f\"Best game: {max(scores)}\")\n",
    "print(f\"Worst game: {min(scores)}\")\n",
    "print(f\"Standard deviation: {np.std(scores):.2f}\")\n",
    "print(f\"\\nGames with score â‰¥10: {sum(1 for s in scores if s >= 10)}/100\")\n",
    "print(f\"Games with score â‰¥20: {sum(1 for s in scores if s >= 20)}/100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e336bce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize score distribution\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(scores, bins=20, edgecolor='black', alpha=0.7)\n",
    "plt.axvline(avg_score, color='r', linestyle='--', linewidth=2, label=f'Mean: {avg_score:.1f}')\n",
    "plt.xlabel('Score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Score Distribution')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(scores, 'b-', alpha=0.6)\n",
    "plt.axhline(avg_score, color='r', linestyle='--', label=f'Mean: {avg_score:.1f}')\n",
    "plt.xlabel('Game Number')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Score per Game')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('../results/notebook_evaluation.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b807298",
   "metadata": {},
   "source": [
    "## 7. Watch the Agent Play\n",
    "\n",
    "Let's watch one game in action (text-based visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90fbf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play one game and show each step\n",
    "game = SnakeGame(BOARD_SIZE, BOARD_SIZE)\n",
    "state = game._get_state_number()\n",
    "game_over = False\n",
    "steps = 0\n",
    "max_steps = 200\n",
    "\n",
    "print(\"Initial State:\")\n",
    "game.display()\n",
    "print(f\"Score: {game.length}\\n\")\n",
    "\n",
    "move_history = []\n",
    "\n",
    "while not game_over and steps < max_steps:\n",
    "    action = agent.select_action(state, training=False)\n",
    "    state, reward, game_over, score = game.step(action)\n",
    "    \n",
    "    move_history.append({\n",
    "        'step': steps,\n",
    "        'action': ['UP', 'RIGHT', 'DOWN', 'LEFT'][action],\n",
    "        'reward': reward,\n",
    "        'score': score\n",
    "    })\n",
    "    \n",
    "    steps += 1\n",
    "\n",
    "print(\"Final State:\")\n",
    "game.display()\n",
    "print(f\"\\nGame Over! Final Score: {score}\")\n",
    "print(f\"Total steps: {steps}\")\n",
    "\n",
    "# Show last 10 moves\n",
    "print(\"\\nLast 10 moves:\")\n",
    "for move in move_history[-10:]:\n",
    "    print(f\"  Step {move['step']}: {move['action']} -> Reward: {move['reward']:+.0f}, Score: {move['score']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4b5829",
   "metadata": {},
   "source": [
    "## 8. Save and Load the Model\n",
    "\n",
    "You can save your trained agent and load it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4194b93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "agent.save('../models/notebook_trained_agent.pkl')\n",
    "\n",
    "print(\"Model saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d58a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "new_agent = QLearningAgent()\n",
    "new_agent.load('../models/notebook_trained_agent.pkl')\n",
    "\n",
    "# Test loaded agent\n",
    "avg, _ = new_agent.evaluate(board_size=BOARD_SIZE, num_episodes=10)\n",
    "print(f\"Loaded agent average score: {avg:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4477e033",
   "metadata": {},
   "source": [
    "## 9. Experiment with Hyperparameters\n",
    "\n",
    "Try different hyperparameter combinations and see how they affect learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db39d8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick comparison of different gamma values\n",
    "gamma_values = [0.6, 0.8, 0.9]\n",
    "results_comparison = {}\n",
    "\n",
    "for gamma in gamma_values:\n",
    "    print(f\"\\nTraining with gamma={gamma}...\")\n",
    "    test_agent = QLearningAgent(gamma=gamma, epsilon=0.2)\n",
    "    test_agent.train(board_size=8, num_episodes=1000, verbose=False)\n",
    "    avg, _ = test_agent.evaluate(board_size=8, num_episodes=25)\n",
    "    results_comparison[gamma] = avg\n",
    "    print(f\"  Average score: {avg:.2f}\")\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.bar([str(g) for g in gamma_values], list(results_comparison.values()), alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('Gamma (Î³)')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Impact of Discount Factor on Performance')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest gamma: {max(results_comparison, key=results_comparison.get)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf98ffd",
   "metadata": {},
   "source": [
    "## 10. Summary and Next Steps\n",
    "\n",
    "### What We Learned:\n",
    "- âœ… How the Snake environment encodes states\n",
    "- âœ… Q-Learning algorithm basics\n",
    "- âœ… Training and evaluating an RL agent\n",
    "- âœ… Analyzing learning progress\n",
    "- âœ… Saving and loading models\n",
    "\n",
    "### Try These Challenges:\n",
    "1. **Increase board size**: Train on a 20x20 board\n",
    "2. **Modify rewards**: Change reward values in `Snake.py`\n",
    "3. **Different exploration**: Try epsilon=0.3 or 0.1\n",
    "4. **Longer training**: Train for 20,000+ episodes\n",
    "5. **Compare agents**: Train multiple agents and compare performance\n",
    "\n",
    "### Advanced Topics:\n",
    "- Implement Deep Q-Networks (DQN) for larger state spaces\n",
    "- Add more features to the state representation\n",
    "- Try different RL algorithms (SARSA, Double Q-Learning)\n",
    "- Create a visual animation of the agent playing"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
